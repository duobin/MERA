{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/train.csv\", header=None, names=[\"lyrics\", \"quadrant\"], skiprows=1)\n",
    "df = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lyrics      Gently hold our hands\\nGently hold our heads o...\n",
       "quadrant                                                    1\n",
       "Name: (213754, -0.6827250804970001, 0.316757791845, Dark Tranquillity, Insanity's Crescendo), dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gently hold our hands Gently hold our heads on high Aimless time in fear new hide Overthrow the plan Confusion lies in all my words Mad is the soul We barricade ourselves in holes of temperament This is the dawning of a new age A heart that beats the wrong way Insanity's crescendo Windcolour, second sight A touch of silence and the violence of dark Illusion span, the aroma of time Shadowlife and the scent of nothingness Infinite fall of instinct Order of one spells deceit Infin\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics = df.lyrics\n",
    "lyrics = \" \".join(\"\".join(lyrics).replace(\"\\n\", \" \").replace(\"\\r\", \" \").split())\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(\n",
    "            lyrics,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = inputs[\"input_ids\"]\n",
    "input_ids = input_ids.unsqueeze(0).type(torch.FloatTensor)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.LSTM(input_size=MAX_LEN, hidden_size=4, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.7616,  0.0000,  0.0000]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, (ht, ct)  = model(input_ids)\n",
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(vocab_size, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 4)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_Model(MAX_LEN, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4910, -0.5687, -0.2336,  0.1143]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lyrics = self.data.lyrics[idx]\n",
    "        lyrics = \" \".join(\"\".join(lyrics).replace(\"\\n\", \" \").replace(\"\\r\", \" \").split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            lyrics,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.type(torch.FloatTensor),\n",
    "            \"labels\": torch.tensor(self.data.quadrant[idx], dtype=torch.long)\n",
    "        } \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>quadrant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gently hold our hands\\nGently hold our heads o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We are the Sun\\nWe are the dead stars\\nWe are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You're out of touch\\nI'm out of time\\nBut I'm ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You finally close the door\\nYou've left open w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>随分先に行ってしまった 光の下のキャラバン\\nトンネルに残響 塞いだ耳 自分嫌いな自分が好き...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  quadrant\n",
       "0  Gently hold our hands\\nGently hold our heads o...         1\n",
       "1  We are the Sun\\nWe are the dead stars\\nWe are ...         1\n",
       "2  You're out of touch\\nI'm out of time\\nBut I'm ...         0\n",
       "3  You finally close the door\\nYou've left open w...         0\n",
       "4  随分先に行ってしまった 光の下のキャラバン\\nトンネルに残響 塞いだ耳 自分嫌いな自分が好き...         3"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate data\n",
    "df_train = pd.read_csv(\"../../data/train.csv\", header=None, names=[\"lyrics\", \"quadrant\"], skiprows=1)\n",
    "df_valid = pd.read_csv(\"../../data/validation.csv\", header=None, names=[\"lyrics\", \"quadrant\"], skiprows=1)\n",
    "df_test = pd.read_csv(\"../../data/test.csv\", header=None, names=[\"lyrics\", \"quadrant\"], skiprows=1)\n",
    "\n",
    "frames = [df_train, df_valid, df_test]\n",
    "\n",
    "df_combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "df_combined[\"quadrant\"] = pd.to_numeric(df_combined[\"quadrant\"])\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset (17427, 2)\n",
      "Train Dataset (13942, 2)\n",
      "Test Dataset (3485, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets\n",
    "trainsize = 0.8\n",
    "trainset = df_combined.sample(frac=trainsize, random_state=42)\n",
    "testset = df_combined.drop(trainset.index).reset_index(drop=True)\n",
    "trainset = trainset.reset_index(drop=True)\n",
    "\n",
    "print(f\"Full Dataset {df_combined.shape}\\n\"\\\n",
    "      f\"Train Dataset {trainset.shape}\\n\"\\\n",
    "      f\"Test Dataset {testset.shape}\")\n",
    "\n",
    "trainset = LyricsDataset(trainset, tokenizer, MAX_LEN)\n",
    "testset = LyricsDataset(testset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]), tensor(3))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][\"input_ids\"].shape, trainset[0][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloaders\n",
    "parameters = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "trainloader = DataLoader(trainset, **parameters)\n",
    "testloader = DataLoader(testset, **parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[ 101., 3414., 3075.,  ..., 1104., 1602.,  102.]],\n",
       " \n",
       "         [[ 101., 1109.,  181.,  ...,    0.,    0.,    0.]],\n",
       " \n",
       "         [[ 101.,  146., 1286.,  ...,    0.,    0.,    0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 101., 1188., 1461.,  ...,  117.,  146.,  102.]],\n",
       " \n",
       "         [[ 101.,  146.,  112.,  ...,    0.,    0.,    0.]],\n",
       " \n",
       "         [[ 101., 2119.,  117.,  ...,    0.,    0.,    0.]]]),\n",
       " 'labels': tensor([2, 3, 1, 3, 2, 3, 3, 0, 2, 0, 0, 2, 0, 1, 1, 0])}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "\n",
    "    model.train()\n",
    "    for _, batch in enumerate(trainloader, 0):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, labels)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=labels.size(0)\n",
    "\n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples\n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.425758957862854\n",
      "Training Accuracy per 5000 steps: 12.5\n",
      "The Total Accuracy for Epoch 0: 19.975613254913213\n",
      "Training Loss Epoch: 1.4555429522324046\n",
      "Training Accuracy Epoch: 19.975613254913213\n",
      "Training Loss per 5000 steps: 1.516968846321106\n",
      "Training Accuracy per 5000 steps: 12.5\n",
      "The Total Accuracy for Epoch 1: 20.585281882082914\n",
      "Training Loss Epoch: 1.450472592077124\n",
      "Training Accuracy Epoch: 20.585281882082914\n",
      "Training Loss per 5000 steps: 1.5273208618164062\n",
      "Training Accuracy per 5000 steps: 0.0\n",
      "The Total Accuracy for Epoch 2: 20.52072873332377\n",
      "Training Loss Epoch: 1.450851172072078\n",
      "Training Accuracy Epoch: 20.52072873332377\n",
      "Training Loss per 5000 steps: 1.5325345993041992\n",
      "Training Accuracy per 5000 steps: 25.0\n",
      "The Total Accuracy for Epoch 3: 19.932577822407115\n",
      "Training Loss Epoch: 1.4522299318138612\n",
      "Training Accuracy Epoch: 19.932577822407115\n",
      "Training Loss per 5000 steps: 1.5450708866119385\n",
      "Training Accuracy per 5000 steps: 18.75\n",
      "The Total Accuracy for Epoch 4: 20.26251613828719\n",
      "Training Loss Epoch: 1.4528530271501716\n",
      "Training Accuracy Epoch: 20.26251613828719\n",
      "Training Loss per 5000 steps: 1.5238300561904907\n",
      "Training Accuracy per 5000 steps: 12.5\n",
      "The Total Accuracy for Epoch 5: 20.061684119925406\n",
      "Training Loss Epoch: 1.452300126262761\n",
      "Training Accuracy Epoch: 20.061684119925406\n",
      "Training Loss per 5000 steps: 1.4741642475128174\n",
      "Training Accuracy per 5000 steps: 25.0\n",
      "The Total Accuracy for Epoch 6: 20.22665327786544\n",
      "Training Loss Epoch: 1.4511925029918689\n",
      "Training Accuracy Epoch: 20.22665327786544\n",
      "Training Loss per 5000 steps: 1.408517599105835\n",
      "Training Accuracy per 5000 steps: 31.25\n",
      "The Total Accuracy for Epoch 7: 19.98278582699756\n",
      "Training Loss Epoch: 1.452142113529214\n",
      "Training Accuracy Epoch: 19.98278582699756\n",
      "Training Loss per 5000 steps: 1.4788150787353516\n",
      "Training Accuracy per 5000 steps: 18.75\n",
      "The Total Accuracy for Epoch 8: 19.946922966575816\n",
      "Training Loss Epoch: 1.4525052832627514\n",
      "Training Accuracy Epoch: 19.946922966575816\n",
      "Training Loss per 5000 steps: 1.3835034370422363\n",
      "Training Accuracy per 5000 steps: 25.0\n",
      "The Total Accuracy for Epoch 9: 20.025821259503658\n",
      "Training Loss Epoch: 1.4523756838993196\n",
      "Training Accuracy Epoch: 20.025821259503658\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validatin the Model\n",
    "def valid(model, testloader):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(testloader, 0):\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, labels)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=labels.size(0)\n",
    "\n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 1.6015472412109375\n",
      "Validation Accuracy per 100 steps: 0.0\n",
      "Validation Loss Epoch: 1.4403768816125502\n",
      "Validation Accuracy Epoch: 21.979913916786227\n",
      "Accuracy on test data = 21.98%\n"
     ]
    }
   ],
   "source": [
    "acc = valid(model, testloader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
